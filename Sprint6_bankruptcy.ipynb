{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from bpitnorm.modules.BatchPitNormalization import BatchPitNorm1d\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "data = np.genfromtxt(\"Data/taiwaneseBankruptcyPred.csv\", delimiter=\",\", skip_header=1)\n",
    "data = data[~(np.isnan(data)).any(axis=1)]\n",
    "data = data[~(np.isinf(data)).any(axis=1)]\n",
    "\n",
    "def data_split(data, seed):\n",
    "  X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(data[:, 1:], data[:, 0], test_size=0.2, random_state=seed)\n",
    "  y_train_raw = y_train_raw.reshape(-1, 1)\n",
    "  y_test_raw = y_test_raw.reshape(-1, 1)\n",
    "\n",
    "  scaler1 = StandardScaler()\n",
    "  scaler_x = scaler1.fit(X_train_raw)\n",
    "  X_train = scaler_x.transform(X_train_raw)\n",
    "  X_test = scaler_x.transform(X_test_raw)\n",
    "\n",
    "  scaler2 = StandardScaler()\n",
    "  scaler_y = scaler2.fit(y_train_raw)\n",
    "  y_train = scaler_y.transform(y_train_raw)\n",
    "  y_test = scaler_y.transform(y_test_raw)\n",
    "\n",
    "  X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "  y_train = torch.tensor(y_train, dtype=torch.float32, device=device).reshape(-1, 1)\n",
    "  X_test = torch.tensor(X_test, dtype=torch.float32).to(device=device)\n",
    "  y_test = torch.tensor(y_test, dtype=torch.float32, device=device).reshape(-1, 1)\n",
    "  return X_train, y_train, X_test, y_test, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_model(model, loops):\n",
    "    model.to(device=device)\n",
    "    acc_lst = np.zeros(loops)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    seeds = range(loops)\n",
    "\n",
    "    for i, seed in zip(range(loops), seeds):\n",
    "        X_train, y_train, X_test, y_test, scaler_y = data_split(data, seed)\n",
    "        torch.manual_seed(seed)\n",
    "        loss_fn = nn.BCEWithLogitsLoss().to(device=device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        train_loader = DataLoader(list(zip(X_train, y_train)), batch_size=150, shuffle=True)\n",
    "        n_epochs = 30\n",
    "        for epoch in range(n_epochs):\n",
    "            model.train()\n",
    "            for x_batch, y_batch in train_loader:\n",
    "                x_batch, y_batch = x_batch.to(device=device), y_batch.to(device=device)\n",
    "                y_pred = model(x_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                loss.backward(retain_graph=True)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "        model.eval()\n",
    "        y_test_inverse = scaler_y.inverse_transform(y_test.cpu())\n",
    "        y_pred = model(X_test, dim=1).argmax(dim=1)\n",
    "        y_pred = torch.softmax(y_pred)\n",
    "        y_pred = scaler_y.inverse_transform(y_pred.cpu().detach().numpy())\n",
    "        acc = (np.around(y_pred) == np.around(y_test_inverse)).mean()\n",
    "        print(\"y_pred =\", y_pred)\n",
    "        print(\"number of 1 predicted:\", np.sum(np.where(np.around(y_pred)==1)))\n",
    "        acc_lst[i] = float(acc)\n",
    "        print(\"accuracy:\", float(acc))\n",
    "    return acc_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m model1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m      2\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m),\n\u001b[1;32m      3\u001b[0m     nn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     nn\u001b[38;5;241m.\u001b[39mDropout(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m),\n\u001b[1;32m      9\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 11\u001b[0m acc1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39mboxplot(acc1, labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel 1\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, loops)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(model, loops):\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     acc_lst \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(loops)\n\u001b[1;32m      7\u001b[0m     torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39mdeterministic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rd/lib/python3.11/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rd/lib/python3.11/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rd/lib/python3.11/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/rd/lib/python3.11/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "model1 = nn.Sequential(\n",
    "    nn.Linear(data.shape[1]-1, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(10, 1))\n",
    "\n",
    "acc1 = train_model(model1, 10)\n",
    "plt.boxplot(acc1, labels=[\"Model 1\"])\n",
    "plt.title(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/batch-pit-normalization/bpitnorm/modules/BatchPitNormalization.py:103: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::quantile.scalar. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987288773/work/aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  q25 = torch.quantile(input=data, q=.25, dim=0)\n",
      "/home/ubuntu/batch-pit-normalization/bpitnorm/modules/BatchPitNormalization.py:104: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::quantile.scalar. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987288773/work/aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  q75 = torch.quantile(input=data, q=.75, dim=0)\n",
      "/home/ubuntu/batch-pit-normalization/bpitnorm/modules/BatchPitNormalization.py:103: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::quantile.scalar. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987288773/work/aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  q25 = torch.quantile(input=data, q=.25, dim=0)\n",
      "/home/ubuntu/batch-pit-normalization/bpitnorm/modules/BatchPitNormalization.py:104: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::quantile.scalar. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987288773/work/aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  q75 = torch.quantile(input=data, q=.75, dim=0)\n",
      "/home/ubuntu/batch-pit-normalization/bpitnorm/modules/BatchPitNormalization.py:103: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::quantile.scalar. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987288773/work/aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  q25 = torch.quantile(input=data, q=.25, dim=0)\n",
      "/home/ubuntu/batch-pit-normalization/bpitnorm/modules/BatchPitNormalization.py:104: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::quantile.scalar. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987288773/work/aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  q75 = torch.quantile(input=data, q=.75, dim=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred = [[0.0318974 ]\n",
      " [0.03189734]\n",
      " [0.03189734]\n",
      " ...\n",
      " [0.03189734]\n",
      " [0.03189734]\n",
      " [0.03189734]]\n",
      "number of 1 predicted: 0\n",
      "accuracy: 0.966275659824047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/batch-pit-normalization/bpitnorm/modules/BatchPitNormalization.py:103: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::quantile.scalar. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987288773/work/aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  q25 = torch.quantile(input=data, q=.25, dim=0)\n",
      "/home/ubuntu/batch-pit-normalization/bpitnorm/modules/BatchPitNormalization.py:104: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::quantile.scalar. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987288773/work/aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  q75 = torch.quantile(input=data, q=.75, dim=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred = [[0.03116407]\n",
      " [0.03116407]\n",
      " [0.03116408]\n",
      " ...\n",
      " [0.03116407]\n",
      " [0.03116407]\n",
      " [0.03116407]]\n",
      "number of 1 predicted: 0\n",
      "accuracy: 0.9633431085043989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/batch-pit-normalization/bpitnorm/modules/BatchPitNormalization.py:103: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::quantile.scalar. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987288773/work/aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  q25 = torch.quantile(input=data, q=.25, dim=0)\n",
      "/home/ubuntu/batch-pit-normalization/bpitnorm/modules/BatchPitNormalization.py:104: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::quantile.scalar. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987288773/work/aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  q75 = torch.quantile(input=data, q=.75, dim=0)\n",
      "/home/ubuntu/batch-pit-normalization/bpitnorm/modules/BatchPitNormalization.py:103: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::quantile.scalar. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987288773/work/aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  q25 = torch.quantile(input=data, q=.25, dim=0)\n",
      "/home/ubuntu/batch-pit-normalization/bpitnorm/modules/BatchPitNormalization.py:104: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::quantile.scalar. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987288773/work/aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  q75 = torch.quantile(input=data, q=.75, dim=0)\n",
      "/home/ubuntu/batch-pit-normalization/bpitnorm/modules/BatchPitNormalization.py:103: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::quantile.scalar. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987288773/work/aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  q25 = torch.quantile(input=data, q=.25, dim=0)\n",
      "/home/ubuntu/batch-pit-normalization/bpitnorm/modules/BatchPitNormalization.py:104: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::quantile.scalar. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987288773/work/aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  q75 = torch.quantile(input=data, q=.75, dim=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred = [[0.0335472]\n",
      " [0.0335472]\n",
      " [0.0335472]\n",
      " ...\n",
      " [0.0335472]\n",
      " [0.0335472]\n",
      " [0.0335472]]\n",
      "number of 1 predicted: 0\n",
      "accuracy: 0.9728739002932552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/batch-pit-normalization/bpitnorm/modules/BatchPitNormalization.py:103: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::quantile.scalar. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987288773/work/aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  q25 = torch.quantile(input=data, q=.25, dim=0)\n",
      "/home/ubuntu/batch-pit-normalization/bpitnorm/modules/BatchPitNormalization.py:104: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::quantile.scalar. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987288773/work/aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  q75 = torch.quantile(input=data, q=.75, dim=0)\n",
      "/home/ubuntu/batch-pit-normalization/bpitnorm/modules/BatchPitNormalization.py:103: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::quantile.scalar. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987288773/work/aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  q25 = torch.quantile(input=data, q=.25, dim=0)\n",
      "/home/ubuntu/batch-pit-normalization/bpitnorm/modules/BatchPitNormalization.py:104: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::quantile.scalar. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987288773/work/aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  q75 = torch.quantile(input=data, q=.75, dim=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred = [[0.03318057]\n",
      " [0.03318057]\n",
      " [0.03318057]\n",
      " ...\n",
      " [0.03318057]\n",
      " [0.03332671]\n",
      " [0.03318057]]\n",
      "number of 1 predicted: 0\n",
      "accuracy: 0.9714076246334311\n",
      "y_pred = [[0.03079743]\n",
      " [0.20353791]\n",
      " [0.03079743]\n",
      " ...\n",
      " [0.03079743]\n",
      " [0.03079743]\n",
      " [0.03079743]]\n",
      "number of 1 predicted: 0\n",
      "accuracy: 0.9618768328445748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/batch-pit-normalization/bpitnorm/modules/BatchPitNormalization.py:103: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::quantile.scalar. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987288773/work/aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  q25 = torch.quantile(input=data, q=.25, dim=0)\n",
      "/home/ubuntu/batch-pit-normalization/bpitnorm/modules/BatchPitNormalization.py:104: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::quantile.scalar. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987288773/work/aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  q75 = torch.quantile(input=data, q=.75, dim=0)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 15\u001b[0m\n\u001b[1;32m      1\u001b[0m model2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m      2\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m95\u001b[39m, \u001b[38;5;241m100\u001b[39m),\n\u001b[1;32m      3\u001b[0m     BatchPitNorm1d(num_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, num_pit_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, take_num_samples_when_full\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, dev\u001b[38;5;241m=\u001b[39mdevice),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     11\u001b[0m     nn\u001b[38;5;241m.\u001b[39mSigmoid())\n\u001b[1;32m     13\u001b[0m model2\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 15\u001b[0m r2Score2, MAE2 \u001b[38;5;241m=\u001b[39m train_model(model2, \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     16\u001b[0m plt\u001b[38;5;241m.\u001b[39mboxplot(r2Score2, labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel 2\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     17\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR2-score\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "model2 = nn.Sequential(\n",
    "    nn.Linear(95, 100),\n",
    "    BatchPitNorm1d(num_features=100, num_pit_samples=200, take_num_samples_when_full=0, dev=device),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.3),\n",
    "    nn.Linear(100, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 1),\n",
    "    nn.Sigmoid())\n",
    "\n",
    "model2.to(device=device)\n",
    "\n",
    "r2Score2, MAE2 = train_model(model2, 5)\n",
    "plt.boxplot(r2Score2, labels=[\"Model 2\"])\n",
    "plt.title(\"R2-score\")\n",
    "plt.show()\n",
    "print(\"Mean house value from all houses: \", round(np.mean(data[:, -1]), -3))\n",
    "plt.boxplot(MAE2, labels=[\"Model 2\"])\n",
    "plt.title(\"Mean absolute error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = nn.Sequential(\n",
    "    BatchPitNorm1d(num_features=8, num_pit_samples=100, take_num_samples_when_full=50, dev=device),\n",
    "    nn.Linear(8, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.BatchNorm1d(10),\n",
    "    # BatchPitNorm1d(num_features=10, num_pit_samples=1000, take_num_samples_when_full=50, dev=device),\n",
    "    nn.Linear(10, 1))\n",
    "model3.to(device=device)\n",
    "\n",
    "r2Score3, MAE3 = train_model(model3, 10)\n",
    "plt.boxplot(r2Score3, labels=[\"Model 3\"])\n",
    "plt.title(\"R2-score\")\n",
    "plt.show()\n",
    "print(\"Mean house value from all houses: \", round(np.mean(data[:, -1]), -3))\n",
    "plt.boxplot(MAE3, labels=[\"Model 3\"])\n",
    "plt.title(\"Mean absolute error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot([r2Score1, r2Score2, r2Score3], labels=[\"Model 1\", \"Model 2\", \"Model 3\"])\n",
    "plt.title(\"R2-score\")\n",
    "plt.show()\n",
    "print(\"Mean house value from all houses: \", round(np.mean(data[:, -1]), -3))\n",
    "plt.boxplot([MAE1, MAE2, MAE3], labels=[\"Model 1\", \"Model 2\", \"Model 3\"])\n",
    "plt.title(\"Mean absolute error\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
