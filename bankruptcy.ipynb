{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from bpitnorm.modules.BatchPitNormalization import BatchPitNorm1d\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "data = np.genfromtxt(\"Data/taiwaneseBankruptcyPred.csv\", delimiter=\",\", skip_header=1)\n",
    "\n",
    "def data_split(data, seed):\n",
    "  data = data[~(np.isnan(data)).any(axis=1)]\n",
    "  data = data[~(np.isinf(data)).any(axis=1)]\n",
    "  X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(data[:, 1:], data[:, 0], test_size=0.2, random_state=seed)\n",
    "  y_train_raw = y_train_raw.reshape(-1, 1)\n",
    "  y_test_raw = y_test_raw.reshape(-1, 1)\n",
    "\n",
    "  scaler1 = StandardScaler()\n",
    "  scaler_x = scaler1.fit(X_train_raw)\n",
    "  X_train = scaler_x.transform(X_train_raw)\n",
    "  X_test = scaler_x.transform(X_test_raw)\n",
    "\n",
    "  scaler2 = StandardScaler()\n",
    "  scaler_y = scaler2.fit(y_train_raw)\n",
    "  y_train = scaler_y.transform(y_train_raw)\n",
    "  y_test = scaler_y.transform(y_test_raw)\n",
    "\n",
    "  X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "  y_train = torch.tensor(y_train, dtype=torch.float32, device=device).reshape(-1, 1)\n",
    "  X_test = torch.tensor(X_test, dtype=torch.float32).to(device=device)\n",
    "  y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1).to(device=device)\n",
    "  return X_train, y_train, X_test, y_test, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_model(model, loops):\n",
    "    model.to(device=device)\n",
    "    acc_lst = np.zeros(loops)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    seeds = range(loops)\n",
    "\n",
    "    for i, seed in zip(range(loops), seeds):\n",
    "        X_train, y_train, X_test, y_test, scaler_y = data_split(data, seed)\n",
    "        torch.manual_seed(seed)\n",
    "        loss_fn = nn.BCELoss().to(device=device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "        train_loader = DataLoader(list(zip(X_train, y_train)), batch_size=150, shuffle=True)\n",
    "        n_epochs = 30\n",
    "        for epoch in range(n_epochs):\n",
    "            model.train()\n",
    "            for x_batch, y_batch in train_loader:\n",
    "                y_pred = model(x_batch)\n",
    "                print(y_pred.shape, y_batch.shape)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                loss.backward(retain_graph=True)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "        model.eval()\n",
    "        y_test_inverse = scaler_y.inverse_transform(y_test.cpu())\n",
    "        y_pred = model(X_test)\n",
    "        y_pred = scaler_y.inverse_transform(y_pred.cpu().detach().numpy())\n",
    "        acc = (y_pred.round() == y_test_inverse).float().mean()\n",
    "        acc_lst[i] = acc = float(acc)\n",
    "    return acc_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m model1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m      2\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m),\n\u001b[1;32m      3\u001b[0m     nn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     nn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[1;32m      9\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 11\u001b[0m \u001b[43mmodel1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m acc1 \u001b[38;5;241m=\u001b[39m train_model(model1, \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     14\u001b[0m plt\u001b[38;5;241m.\u001b[39mboxplot(acc1, labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel 1\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/rd/lib/python3.11/site-packages/torch/nn/modules/module.py:911\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    895\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \n\u001b[1;32m    897\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 911\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rd/lib/python3.11/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rd/lib/python3.11/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/rd/lib/python3.11/site-packages/torch/nn/modules/module.py:911\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    895\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \n\u001b[1;32m    897\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 911\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "model1 = nn.Sequential(\n",
    "    nn.Linear(data.shape[1]-1, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 10),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 1))\n",
    "\n",
    "model1.cuda()\n",
    "\n",
    "acc1 = train_model(model1, 10)\n",
    "plt.boxplot(acc1, labels=[\"Model 1\"])\n",
    "plt.title(\"Absolute accuracy\")\n",
    "plt.show()\n",
    "print(f\"Mean house value from all houses: {np.mean(data[:, -1])}\")\n",
    "plt.boxplot(acc1/data.shape[0], labels=[\"Model 1\"])\n",
    "plt.title(\"Procentage accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = nn.Sequential(\n",
    "    BatchPitNorm1d(num_features=8, num_pit_samples=200, take_num_samples_when_full=0, dev=device),\n",
    "    nn.Linear(8, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.3),\n",
    "    nn.Linear(100, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 1))\n",
    "\n",
    "model2.to(device=device)\n",
    "\n",
    "r2Score2, MAE2 = train_model(model2, 10)\n",
    "plt.boxplot(r2Score2, labels=[\"Model 2\"])\n",
    "plt.title(\"R2-score\")\n",
    "plt.show()\n",
    "print(\"Mean house value from all houses: \", round(np.mean(data[:, -1]), -3))\n",
    "plt.boxplot(MAE2, labels=[\"Model 2\"])\n",
    "plt.title(\"Mean absolute error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = nn.Sequential(\n",
    "    BatchPitNorm1d(num_features=8, num_pit_samples=100, take_num_samples_when_full=50, dev=device),\n",
    "    nn.Linear(8, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.BatchNorm1d(10),\n",
    "    # BatchPitNorm1d(num_features=10, num_pit_samples=1000, take_num_samples_when_full=50, dev=device),\n",
    "    nn.Linear(10, 1))\n",
    "model3.to(device=device)\n",
    "\n",
    "r2Score3, MAE3 = train_model(model3, 10)\n",
    "plt.boxplot(r2Score3, labels=[\"Model 3\"])\n",
    "plt.title(\"R2-score\")\n",
    "plt.show()\n",
    "print(\"Mean house value from all houses: \", round(np.mean(data[:, -1]), -3))\n",
    "plt.boxplot(MAE3, labels=[\"Model 3\"])\n",
    "plt.title(\"Mean absolute error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot([r2Score1, r2Score2, r2Score3], labels=[\"Model 1\", \"Model 2\", \"Model 3\"])\n",
    "plt.title(\"R2-score\")\n",
    "plt.show()\n",
    "print(\"Mean house value from all houses: \", round(np.mean(data[:, -1]), -3))\n",
    "plt.boxplot([MAE1, MAE2, MAE3], labels=[\"Model 1\", \"Model 2\", \"Model 3\"])\n",
    "plt.title(\"Mean absolute error\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
